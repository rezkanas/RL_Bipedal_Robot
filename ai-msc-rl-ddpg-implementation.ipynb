{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ac16a5-d21a-46e8-9e13-e0ba734cc80c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-metal -U #decorator==4.4.2  # gymnasium==0.26.3 #gymnasium[box2d] tensorboard keras tensorflow moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53df030-2748-4307-ac0b-7e748784753c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import gymnasium as gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Setting to CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a350bb2-2454-45ae-b6a6-e062382d5d04",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "print(gymnasium.__version__)\n",
    "\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33485d2b-efe8-4efc-976d-fa72160f5540",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set seed across random and numpy libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_env(seed, hardcore: bool = False):\n",
    "    # use gymnasium to create an BipedalWalker-v3 environment\n",
    "    env = gym.make(\"BipedalWalker-v3\", hardcore=hardcore, render_mode=\"rgb_array_list\")\n",
    "\n",
    "    # Reset Environment\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "class Networks:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,\n",
    "        critic_network: List[List[int]],\n",
    "        actor_network: List[int],\n",
    "        agent=None,\n",
    "        twin=False,\n",
    "    ):\n",
    "        # example:\n",
    "        #  actor_network [96, 96]\n",
    "        # critic_network [[],     action network, maximum one element\n",
    "        #                 [96, 96] state network, two element\n",
    "        #                 [192]   after Concatenation,  maximum one element  ]\n",
    "\n",
    "        # define the backbone of your network\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # define the network architecture\n",
    "        self.critic_network = critic_network\n",
    "        self.actor_network = actor_network\n",
    "\n",
    "        # passing an agent\n",
    "        self.agent = agent\n",
    "\n",
    "        # when using this class for TD3, Twin shall be True\n",
    "        self.twin = twin\n",
    "\n",
    "    # Changes have been made here.\n",
    "    def log_model(self, model: tf.keras.Model, model_type: str):\n",
    "        try:\n",
    "            logdir = os.path.join(\n",
    "                \"logs\",\n",
    "                self.agent.name,\n",
    "                model_type,\n",
    "                datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "            TensorBoardCall = tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=logdir, update_freq=1\n",
    "            )\n",
    "            TensorBoardCall.set_model(model=model)\n",
    "        except:  # this is not good practise, what error were you encountering? execpt should be specific\n",
    "            pass\n",
    "\n",
    "    def generate_actor_network(self):\n",
    "        if self.backbone in [\"MLP\", \"LSTM\"]:\n",
    "            inputs = layers.Input(\n",
    "                shape=(num_parameters_recorded_per_states), name=\"state_input\"\n",
    "            )\n",
    "            out = layers.Dense(\n",
    "                self.actor_network[0], activation=\"relu\", name=\"MLP_state_layer\"\n",
    "            )(inputs)\n",
    "\n",
    "            if self.backbone == \"LSTM\":\n",
    "                # expand dimensions of input layers to have the timestamps\n",
    "                out = layers.Lambda(\n",
    "                    lambda x: tf.expand_dims(x, 1), name=\"dimension_expand\"\n",
    "                )(out)\n",
    "                out = layers.LSTM(\n",
    "                    self.actor_network[1],\n",
    "                    return_sequences=True,\n",
    "                    return_state=True,\n",
    "                    dropout=0.5,\n",
    "                    name=\"LSTM_backbone_1\",\n",
    "                )(\n",
    "                    out\n",
    "                )  # additional layer\n",
    "                out = layers.LSTM(\n",
    "                    self.actor_network[1], dropout=0.5, name=\"LSTM_backbone_2\"\n",
    "                )(out)\n",
    "\n",
    "            else:\n",
    "                out = layers.Dense(\n",
    "                    self.actor_network[1], activation=\"relu\", name=\"MLP_backbone\"\n",
    "                )(out)\n",
    "\n",
    "            # actions are from -1 to 1 so tanh is a natural choice, sigmoid can be used for scaled values\n",
    "            outputs = layers.Dense(\n",
    "                num_actions_per_step, activation=\"tanh\", name=\"MLP_last_layer\"\n",
    "            )(out)\n",
    "            model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "            # Changes have been made here.\n",
    "            self.log_model(model=model, model_type=\"actor\")\n",
    "\n",
    "            return model\n",
    "\n",
    "        elif self.backbone == \"Transformers\":\n",
    "            pass\n",
    "\n",
    "    def generate_critic_network(self):\n",
    "        if self.backbone in [\"MLP\", \"LSTM\"]:\n",
    "            # Input the State\n",
    "            state_input = layers.Input(\n",
    "                shape=num_parameters_recorded_per_states, name=\"state_input\"\n",
    "            )\n",
    "\n",
    "            if len(self.critic_network[0]) >= 1:\n",
    "                state_out = layers.Dense(\n",
    "                    self.critic_network[0][0],\n",
    "                    activation=\"relu\",\n",
    "                    name=\"MLP_state_layer\",\n",
    "                )(state_input)\n",
    "            else:\n",
    "                state_out = state_input\n",
    "\n",
    "            # Input the Action\n",
    "            action_input = layers.Input(shape=num_actions_per_step, name=\"action_input\")\n",
    "            action_output = layers.Dense(\n",
    "                self.critic_network[1][0], activation=\"relu\", name=\"MLP_action_layer\"\n",
    "            )(action_input)\n",
    "\n",
    "            if self.backbone == \"LSTM\":\n",
    "                # expand dimensions of input layers to count for the timesteps\n",
    "                out = layers.Lambda(lambda x: tf.expand_dims(x, 1))(action_output)\n",
    "                out = layers.LSTM(\n",
    "                    self.critic_network[1][1],\n",
    "                    return_sequences=True,\n",
    "                    return_state=True,\n",
    "                    dropout=0.5,\n",
    "                    name=\"LSTM_action_layer_backbone_1\",\n",
    "                )(\n",
    "                    out\n",
    "                )  # additional layer\n",
    "                action_output = layers.LSTM(\n",
    "                    self.critic_network[1][1],\n",
    "                    dropout=0.5,\n",
    "                    name=\"LSTM_action_layer_backbone_2\",\n",
    "                )(out)\n",
    "\n",
    "            else:\n",
    "                action_output = layers.Dense(\n",
    "                    self.critic_network[1][1],\n",
    "                    activation=\"relu\",\n",
    "                    name=\"MLP_action_layer_backbone\",\n",
    "                )(action_output)\n",
    "\n",
    "            # Both are passed through separate layer before concatenating\n",
    "            concat = layers.Concatenate()([state_out, action_output])\n",
    "\n",
    "            out = layers.Dense(\n",
    "                self.critic_network[2][0], activation=\"relu\", name=\"after_Concatenation\"\n",
    "            )(concat)\n",
    "            outputs = layers.Dense(1)(out)\n",
    "\n",
    "            # Outputs single value for give state-action\n",
    "            model = tf.keras.Model([state_input, action_input], outputs)\n",
    "            # Changes have been made here.\n",
    "            self.log_model(model=model, model_type='critic')\n",
    "\n",
    "            return model\n",
    "\n",
    "        elif self.backbone == \"Transformers\":\n",
    "            pass\n",
    "\n",
    "\n",
    "class Actor(keras.Model):\n",
    "    \"\"\"Creates an actor network\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        state_dim: Observation environment dimensions, this will be used to determine network input size.\n",
    "        action_dim: Action dimensions, this will be used to determine network output.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.inp_layer = Dense(\n",
    "            state_dim,\n",
    "        )\n",
    "        self.layer_1 = Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "        )\n",
    "        self.layer_2 = Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "        )\n",
    "        # self.layer_3 = Dense(200, activation='relu',)\n",
    "        self.out_layer = Dense(\n",
    "            action_dim,\n",
    "            activation='tanh',\n",
    "        )\n",
    "\n",
    "    def call(self, observation):\n",
    "        x = self.inp_layer(observation)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        # x = self.layer_3(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticDDPG(keras.Model):\n",
    "    \"\"\"Creates two critic networks\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: The dimensions of the state the environment will produce.\n",
    "                The first input for the network.\n",
    "            action_dim: The dimensions of the actions the environment can take.\n",
    "                The second input for the network.\n",
    "        \"\"\"\n",
    "        super(CriticDDPG, self).__init__()\n",
    "        # The First Critic NN\n",
    "        self.inp_layer_1 = Dense(\n",
    "            state_dim + action_dim,\n",
    "        )\n",
    "        self.layer_1_1 = Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "        )\n",
    "        self.layer_2_1 = Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "        )\n",
    "        # self.layer_3_1 = Dense(200, activation='relu',)\n",
    "        self.out_layer_1 = Dense(\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    def call(self, observation_plus_action):\n",
    "        x0 = tf.concat(observation_plus_action, 1)\n",
    "        # forward propagate the first NN\n",
    "        x1 = self.inp_layer_1(x0)\n",
    "        x1 = self.layer_1_1(x1)\n",
    "        x1 = self.layer_2_1(x1)\n",
    "        # x1 = self.layer_3_1(x1)\n",
    "        x1 = self.out_layer_1(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "class NetworksV2(Networks):\n",
    "    def __init__(self):\n",
    "        super(Networks, self)\n",
    "\n",
    "    def generate_critic_network(self, name: str = 'critic'):\n",
    "        critic = CriticDDPG(num_parameters_recorded_per_states, num_actions_per_step)\n",
    "        self.log_model(critic, name)\n",
    "        return critic\n",
    "\n",
    "    def generate_actor_network(self, name: str = 'actor'):\n",
    "        actor = Actor(num_parameters_recorded_per_states, num_actions_per_step)\n",
    "        self.log_model(actor, name)\n",
    "        return actor\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(\n",
    "        self, buffer_size: int, batch_size: int, td_error_prioritization: bool = False\n",
    "    ):\n",
    "        \"\"\"This class creates np zeroes for the buffer of all properties in SARS and updates them with a FIFO method.\n",
    "        There are also two sampling methods, uniform or prioritised based on td-loss.\n",
    "        \"\"\"\n",
    "        self.capacity = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.td_error_prioritization = td_error_prioritization\n",
    "\n",
    "        # logs number of times the replay.record has been called\n",
    "        self.counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros(\n",
    "            (self.capacity, num_parameters_recorded_per_states)\n",
    "        )\n",
    "        self.action_buffer = np.zeros((self.capacity, num_actions_per_step))\n",
    "        self.reward_buffer = np.zeros((self.capacity, 1))\n",
    "        self.next_state_buffer = np.zeros(\n",
    "            (self.capacity, num_parameters_recorded_per_states)\n",
    "        )\n",
    "        # Changes have been made here.\n",
    "        self.terminal_or_truncated = np.ones((self.capacity, 1), dtype=bool)\n",
    "        if self.td_error_prioritization:\n",
    "            self.priority_value = np.zeros(self.capacity)\n",
    "\n",
    "    def store(self, observation, td_loss: float):\n",
    "        # Get sampling range\n",
    "        index = self.counter % self.capacity\n",
    "\n",
    "        (\n",
    "            self.state_buffer[index],\n",
    "            self.action_buffer[index],\n",
    "            self.reward_buffer[index],\n",
    "            self.next_state_buffer[index],\n",
    "            self.terminal_or_truncated[index],  # Changes have been made here.\n",
    "        ) = observation\n",
    "\n",
    "        if self.td_error_prioritization:\n",
    "            self.priority_value[index] = td_loss\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "    def mini_batch_sample(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.counter, self.capacity)\n",
    "\n",
    "        if self.td_error_prioritization:\n",
    "            weights = self.priority_value[:record_range] / np.sum(\n",
    "                self.priority_value[:record_range]\n",
    "            )\n",
    "            batch_indices = np.random.choice(record_range, self.batch_size, p=weights)\n",
    "        else:\n",
    "            # Randomly sample indices\n",
    "            batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        # Changes have been made here.\n",
    "        terminal_or_truncated_batch = tf.convert_to_tensor(\n",
    "            self.terminal_or_truncated[batch_indices]\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            reward_batch,\n",
    "            next_state_batch,\n",
    "            terminal_or_truncated_batch,\n",
    "        )\n",
    "\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(\n",
    "        self, action_size: int, mu: float = 0, theta: float = 0.15, sigma: float = 0.2\n",
    "    ):\n",
    "        # Initialize parameters and noise process.\n",
    "        self.mu = mu * np.ones(action_size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.noise_state = copy.copy(self.mu)\n",
    "        self.noise_len = len(self.noise_state)\n",
    "\n",
    "    def sample(self):\n",
    "        # return a noise sample.\n",
    "        # Changes have been made here.\n",
    "        delta_noise = self.theta * (\n",
    "            self.mu - self.noise_state\n",
    "        ) + self.sigma * np.random.normal(\n",
    "            0, 0.5, self.noise_len\n",
    "        )  # np.random.randn(self.noise_len)\n",
    "        self.noise_state = self.noise_state + delta_noise\n",
    "        return self.noise_state\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp)\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as outp:\n",
    "        object_ = pickle.load(outp)\n",
    "    return object_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1d5b8a-ed78-4d53-a2f6-a8ebb163889f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# set up\n",
    "env = create_env(seed)\n",
    "num_parameters_recorded_per_states = env.observation_space.shape[0]\n",
    "num_actions_per_step = env.action_space.shape[0]\n",
    "upper_limit = env.action_space.high[0]\n",
    "lower_limit = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af351f7-091b-4daa-8698-62575f9cfc9b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    # Changes have been made here.\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,  # Network setup variable\n",
    "        critic_network: List[List[int]],  # Network setup variable\n",
    "        actor_network: List[int],  # Network setup variable\n",
    "        tdl_priority: bool,  # Sampling variable\n",
    "        buffer_size: int,  # Sampling variable\n",
    "        batch_size: int,  # Sampling variable\n",
    "        critic_lr: float,  # Network learning parameter\n",
    "        actor_lr: float,  # Network learning parameter\n",
    "        gamma: float,  # Network learning parameter\n",
    "        beta: float,  # Network learning parameter\n",
    "        ou_mu: float = 0,  # Exploration noise parameter\n",
    "        ou_theta: float = 0.15,  # Exploration noise parameter\n",
    "        ou_sigma: float = 0.2,  # Exploration noise parameter\n",
    "        name: str = 'basic',  # appends to agent name for folder structure purposes\n",
    "        network_type: object = Networks,  # Which class to use Networks or NetworksV2\n",
    "    ):\n",
    "        self.name = f\"DDPG_{backbone}_{name}\"\n",
    "\n",
    "        # for creating\n",
    "        self.tdl_priority = tdl_priority\n",
    "\n",
    "        # initialise a replay buffer D\n",
    "        self.buffer = ReplayBuffer(buffer_size, batch_size, tdl_priority)\n",
    "\n",
    "        # define the network\n",
    "        ntk = (\n",
    "            network_type(backbone, critic_network, actor_network, agent=self)\n",
    "            if network_type.__name__ == 'Networks'\n",
    "            else network_type()\n",
    "        )\n",
    "        print(f'Building networks with class {network_type.__name__}.')\n",
    "\n",
    "        # create target and regular actor network\n",
    "        self.actor_network = ntk.generate_actor_network()\n",
    "        self.target_actor = ntk.generate_actor_network(name='target_actor')\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_network.get_weights())\n",
    "\n",
    "        # create target and regular critic network\n",
    "        self.critic_network = ntk.generate_critic_network()\n",
    "        self.target_critic = ntk.generate_critic_network(name='target_critic')\n",
    "        # Making the weights equal initially\n",
    "        self.target_critic.set_weights(self.critic_network.get_weights())\n",
    "\n",
    "        # Learning rate for actor-critic models\n",
    "        # from tensorflow.keras.optimizers.legacy import Adam\n",
    "        self.critic_optimizer = tf.keras.optimizers.legacy.Adam(critic_lr)\n",
    "        self.actor_optimizer = tf.keras.optimizers.legacy.Adam(actor_lr)\n",
    "\n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # target learning rate\n",
    "        self.beta = beta\n",
    "\n",
    "        # initiate noise\n",
    "        self.noise = OrnsteinUhlenbeckNoise(\n",
    "            num_actions_per_step, ou_mu, ou_theta, ou_sigma\n",
    "        )\n",
    "\n",
    "    def train(self, observation_tuple):\n",
    "        # calculate TD-error when replay buffer is using TD-error prioritization\n",
    "        if self.tdl_priority:\n",
    "            prev_state, action, reward, state, terminal_or_truncated = observation_tuple\n",
    "            state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = tf.expand_dims(tf.convert_to_tensor(action), 0)\n",
    "            # Changes have been made here. Setting terminal state action predictions to 0.\n",
    "            # True value, y\n",
    "            q_s_dash_best_a_dash = (\n",
    "                self.target_critic([state, self.target_actor(state)])\n",
    "                if not terminal_or_truncated\n",
    "                else 0\n",
    "            )\n",
    "            y = reward + self.gamma * q_s_dash_best_a_dash\n",
    "            # Prediction, y_hat\n",
    "            y_chapeau = self.critic_network([state, action])\n",
    "            # td_loss is only used for tdl_priority!\n",
    "            td_loss = tf.math.abs(tf.math.subtract(y, y_chapeau)).numpy()\n",
    "            # changed this to inverse loss, aka accuracy kind of.\n",
    "            # td_loss = (1 - (tf.math.abs(tf.math.square(y) - tf.math.square(y_chapeau)) / (tf.math.square(y) + tf.math.square(y_chapeau)))).numpy()\n",
    "\n",
    "        else:\n",
    "            td_loss = 0\n",
    "\n",
    "        # store transition in buffer\n",
    "        self.buffer.store(observation_tuple, td_loss)\n",
    "\n",
    "        if self.buffer.counter > 10_000:\n",
    "            # minibatch sampled from buffer\n",
    "            (\n",
    "                state_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "                next_state_batch,\n",
    "                terminal_or_truncated,\n",
    "            ) = self.buffer.mini_batch_sample()  # Changes have been made here\n",
    "\n",
    "            # perform gradient descent step for critic network\n",
    "            with tf.GradientTape() as tape:\n",
    "                # tape.watch(self.critic_network.trainable_variables) # watch_accessed_variables=False\n",
    "                target_actions = self.target_actor(next_state_batch, training=True)\n",
    "                # True value for the action state.\n",
    "                q_s_dash_best_a_dash_tensor = tf.Variable(\n",
    "                    self.target_critic(\n",
    "                        [next_state_batch, target_actions], training=True\n",
    "                    )\n",
    "                )\n",
    "                # Changes have been made here. Updating max Q(s', a') for terminal states to be 0.\n",
    "                q_s_dash_best_a_dash_tensor.assign(\n",
    "                    tf.where(\n",
    "                        terminal_or_truncated == True, 0, q_s_dash_best_a_dash_tensor\n",
    "                    )\n",
    "                )\n",
    "                q_s_dash_best_a_dash_tensor = tf.convert_to_tensor(\n",
    "                    q_s_dash_best_a_dash_tensor\n",
    "                )\n",
    "                y = tf.math.add(\n",
    "                    reward_batch,\n",
    "                    tf.math.scalar_mul(self.gamma, q_s_dash_best_a_dash_tensor),\n",
    "                )\n",
    "\n",
    "                # Agent's prediction for the current action-state value\n",
    "                critic_value = self.critic_network(\n",
    "                    [state_batch, action_batch], training=True\n",
    "                )\n",
    "                # MSE loss\n",
    "                critic_loss = tf.math.reduce_mean(\n",
    "                    tf.math.square(tf.math.subtract(y, critic_value))\n",
    "                )\n",
    "\n",
    "            # Update agent's critic's parameters only\n",
    "            critic_grad = tape.gradient(\n",
    "                critic_loss, self.critic_network.trainable_variables\n",
    "            )\n",
    "            self.critic_optimizer.apply_gradients(\n",
    "                zip(critic_grad, self.critic_network.trainable_variables)\n",
    "            )\n",
    "\n",
    "            # Changes have been made here. The order of the networks updates are swapped.\n",
    "            # perform gradient descent step for actor network\n",
    "            with tf.GradientTape() as tape:\n",
    "                # tape.watch(self.actor_network.trainable_variables)  # watch_accessed_variables=False\n",
    "                actions = self.actor_network(state_batch, training=True)\n",
    "                critic_value = self.critic_network(\n",
    "                    [state_batch, actions], training=True\n",
    "                )\n",
    "                # Used negative value for maximizing the value given by the critic of the input action\n",
    "                actor_loss = tf.math.scalar_mul(-1, tf.math.reduce_mean(critic_value))\n",
    "\n",
    "            actor_grad = tape.gradient(\n",
    "                actor_loss, self.actor_network.trainable_variables\n",
    "            )\n",
    "\n",
    "            self.actor_optimizer.apply_gradients(\n",
    "                zip(actor_grad, self.actor_network.trainable_variables)\n",
    "            )\n",
    "\n",
    "            # update target actor network parameters\n",
    "            w2 = self.target_actor.get_weights()\n",
    "            w1 = self.actor_network.get_weights()\n",
    "            self.target_actor.set_weights(\n",
    "                [self.beta * x + (1 - self.beta) * y for x, y in zip(w1, w2)]\n",
    "            )\n",
    "\n",
    "            # update target critic network parameters\n",
    "            theta2 = self.target_critic.get_weights()\n",
    "            theta1 = self.critic_network.get_weights()\n",
    "            self.target_critic.set_weights(\n",
    "                [self.beta * x + (1 - self.beta) * y for x, y in zip(theta1, theta2)]\n",
    "            )\n",
    "\n",
    "    def predict_action(self, state, env):\n",
    "        if self.buffer.counter >= 50_000:\n",
    "            # using actor network, predict actions value\n",
    "            selected_actions = self.actor_network(state)\n",
    "            # print((selected_actions), len(selected_actions.numpy()), selected_actions.numpy(), 'action stats')\n",
    "            # calculating noise\n",
    "            self.prev_ou_level = self.noise.sample()\n",
    "            self.prev_ou_level = np.expand_dims(self.prev_ou_level, 0)\n",
    "            # print((self.prev_ou_level), len(self.prev_ou_level), self.prev_ou_level, 'noise stats')\n",
    "            # print(np.add(self.prev_ou_level, selected_actions.numpy()))\n",
    "            # Adding noise to action\n",
    "\n",
    "            selected_actions = np.add(\n",
    "                self.prev_ou_level, selected_actions.numpy()\n",
    "            )  # selected_actions + self.prev_ou_level\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            action = np.clip(selected_actions, lower_limit, upper_limit)  # [0]\n",
    "            # print(tf.convert_to_tensor(action)[0])\n",
    "            return tf.convert_to_tensor(action)[0]\n",
    "\n",
    "        else:\n",
    "            # explore till replay buffer is full\n",
    "            return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac73a6ea-45e8-484f-810f-a080121716c8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    env, agent, total_n_episodes, seed, training=True, record_modulo: int = 100\n",
    "):\n",
    "    entered = True\n",
    "    # colect the 3 top performance\n",
    "    three_best_performing_agent = {x: -200 for x in range(3)}\n",
    "\n",
    "    # path to save videos\n",
    "    top_3_agents_path = os.path.join(\n",
    "        agent.name, 'top_3_agents', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    )\n",
    "    capped_cubic_path = os.path.join(\n",
    "        agent.name,\n",
    "        'capped_cubic_video',\n",
    "        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    )\n",
    "\n",
    "    # record statistics\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=total_n_episodes)\n",
    "\n",
    "    # set seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    record_episode_actions_log = defaultdict(list)\n",
    "\n",
    "    # count number of times the agents falls\n",
    "    falls_counter = 0\n",
    "    timesteps_in_environment = 0\n",
    "    # Train the agent for N episodes\n",
    "    for episode in range(total_n_episodes):\n",
    "        # collect actions of some episode to use them to analyze algorithm performance\n",
    "        record_episode_actions = False\n",
    "        if episode % record_modulo == 0:\n",
    "            record_episode_actions = True\n",
    "\n",
    "        # convert the observation to tensor to run fast in colab gpu\n",
    "        # and add one dimension to fit with the input shape requirement of the network\n",
    "        prev_state = tf.expand_dims(tf.convert_to_tensor(env.reset(seed=seed)[0]), 0)\n",
    "\n",
    "        episode_reward = 0\n",
    "        step_starting_index = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "            action = agent.predict_action(prev_state, env)\n",
    "\n",
    "            if record_episode_actions:\n",
    "                record_episode_actions_log[episode].append(action)\n",
    "\n",
    "            # Receive state and reward from environment.\n",
    "            state, reward, terminal, truncated, info = env.step(action)\n",
    "\n",
    "            if training:\n",
    "                agent.train((prev_state, action, reward, state, terminal))\n",
    "\n",
    "            if reward == -100:\n",
    "                falls_counter += 1\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            if terminal or truncated:\n",
    "                if (\n",
    "                    min(three_best_performing_agent.values()) < episode_reward\n",
    "                ) and episode > 100:\n",
    "                    # find the index of the previously less performing agent and its path\n",
    "                    index = [\n",
    "                        i\n",
    "                        for i in three_best_performing_agent\n",
    "                        if three_best_performing_agent[i]\n",
    "                        == min(three_best_performing_agent.values())\n",
    "                    ][0]\n",
    "                    remove_video_path = (\n",
    "                        top_3_agents_path + '/rl-video-episode-' + str(index) + '.mp4'\n",
    "                    )\n",
    "\n",
    "                    # remove videos where agents performance is no longer in the top 3 best performing agents\n",
    "                    if os.path.isfile(remove_video_path):\n",
    "                        os.remove(remove_video_path)\n",
    "\n",
    "                    # remove it from the list and add the new agent\n",
    "                    three_best_performing_agent.pop(index)\n",
    "                    three_best_performing_agent[episode] = episode_reward\n",
    "\n",
    "                    # save its video\n",
    "                    save_video(\n",
    "                        env.render(),\n",
    "                        top_3_agents_path,\n",
    "                        fps=env.metadata[\"render_fps\"],\n",
    "                        episode_trigger=lambda x: x == x,\n",
    "                        step_starting_index=step_starting_index,\n",
    "                        episode_index=episode,\n",
    "                    )\n",
    "                    entered = True\n",
    "\n",
    "                if not entered:\n",
    "                    # save capped cubic episode indices 0, 1, 4, 8, 27, …, K^3, …, 729, 1000, 2000, 3000, …\n",
    "                    save_video(\n",
    "                        env.render(),\n",
    "                        capped_cubic_path,\n",
    "                        fps=env.metadata[\"render_fps\"],\n",
    "                        step_starting_index=step_starting_index,\n",
    "                        episode_index=episode,\n",
    "                    )\n",
    "                entered = False\n",
    "                break\n",
    "\n",
    "            prev_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "\n",
    "        # share progress every 100 episodes about the last 100 episodes\n",
    "        # if episode % 100 == 0 and episode != 0:\n",
    "\n",
    "        R_100MA_episode_rewards = np.round(\n",
    "            np.mean(np.array(env.return_queue).flatten()[:-100]), 2\n",
    "        )\n",
    "        R_100MA_episode_lengths = np.round(\n",
    "            np.mean(np.array(env.length_queue).flatten()[:-100]), 2\n",
    "        )\n",
    "        # print(type(R_100MA_episode_rewards), np.round(R_100MA_episode_rewards))\n",
    "        print(\n",
    "            f\"In episode {episode}, the reward is {round(episode_reward, 2)},\"\n",
    "            f\"\\nthe 100 moving avg reward is ==> {np.round(R_100MA_episode_rewards)},*\"\n",
    "            f\"\\nAvg episode length is ==> {R_100MA_episode_lengths}\"\n",
    "            f\"\\n{agent.buffer.counter} steps have been completed\"\n",
    "            f\"\\nand {steps} timesteps were taken this episode.\\n\"\n",
    "        )\n",
    "\n",
    "    return agent, env, record_episode_actions_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b6ca86-af95-46ed-bc4d-f1402e8dc1a9",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_ddpg = DDPGAgent(\n",
    "    backbone=\"MLP\",\n",
    "    critic_network=[[400], [400, 300], [200]],  # 256, 256\n",
    "    actor_network=[400, 300],  # 256, 256\n",
    "    tdl_priority=True,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=64,\n",
    "    critic_lr=0.0004,  # 0.0004\n",
    "    actor_lr=0.0004,  # 0.0004\n",
    "    gamma=0.98,\n",
    "    beta=0.005,  # tau, target_lr # 0.005\n",
    "    ou_mu=0,\n",
    "    ou_theta=0.3,  # 0.3\n",
    "    ou_sigma=0.8,  # 1.0\n",
    "    name='50k_buffer',\n",
    "    network_type=NetworksV2,\n",
    ")\n",
    "\n",
    "total_n_episodes = 1_000\n",
    "ddpg_env = create_env(seed)\n",
    "start_time = time.time()\n",
    "ddpg_agent, ddpg_env, ddpg_record_episode_actions_log = run_experiment(\n",
    "    ddpg_env, agent_ddpg, total_n_episodes, seed\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'{total_n_episodes} took {round(end_time - start_time, 2)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc00ebcd-1cb5-47c4-a597-9c9c9786145d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    # os.mkdir(filename)\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp)\n",
    "\n",
    "\n",
    "ep_rewards = np.concatenate(list(ddpg_env.return_queue))\n",
    "ep_duration = np.concatenate(list(ddpg_env.length_queue))\n",
    "ep_actions = ddpg_record_episode_actions_log\n",
    "\n",
    "job_timestamp = '20230423-094145'\n",
    "local_path = '/path/to/save/to'\n",
    "\n",
    "video_render_path = f'{local_path}/top_3_agents/{job_timestamp}'\n",
    "\n",
    "\n",
    "save_object(ep_rewards, f\"{video_render_path}/ep_rewards_{total_n_episodes}\")\n",
    "save_object(ep_duration, f\"{video_render_path}/ep_duration_{total_n_episodes}\")\n",
    "save_object(ep_actions, f\"{video_render_path}/ep_actions_{total_n_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc32d88-3b3d-4868-8ed2-965b69aa0b20",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def success_rate(env):\n",
    "    # calculate the success rate of an agent out of all training epsiodes\n",
    "    # a successful trail is when the reward exceed 200 ???\n",
    "    reward_queue = np.array(env.return_queue).flatten()\n",
    "    return len(np.where(reward_queue >= 200)) / len(reward_queue)\n",
    "\n",
    "\n",
    "def R100_MA(env, total_n_episodes, name):\n",
    "    # calculate the moving average of reward for every 100 episodes\n",
    "    R100_MA = np.empty(total_n_episodes, dtype=float)\n",
    "\n",
    "    for i in range(total_n_episodes):\n",
    "        R100_MA[i] = np.mean(np.array(env.return_queue).flatten()[i : i + 100])\n",
    "\n",
    "    MA = {}\n",
    "    MA[name] = R100_MA\n",
    "    return MA\n",
    "\n",
    "\n",
    "def plot_R100_MA(*R100_MA_env_Z):\n",
    "    # plot 100-episodes-moving-average of reward and scatter point of rewards\n",
    "    for R100_MA_env in R100_MA_env_Z:\n",
    "        reward_env = np.array(R100_MA_env[1].return_queue)\n",
    "        for agent_name, value in R100_MA_env[0].items():\n",
    "            plt.plot(value, 'r', label='best fit')\n",
    "            plt.scatter(np.arange(0, len(reward_env), 1), reward_env, label='rewards')\n",
    "            plt.legend(\n",
    "                bbox_to_anchor=(1.04, 0.0, 0.2, 1),\n",
    "                loc=\"lower left\",  # agent_name,\n",
    "                borderaxespad=0,\n",
    "                mode='expand',\n",
    "            )\n",
    "            plt.xlabel('episodes')\n",
    "            plt.ylabel('reward')\n",
    "\n",
    "    plt.title('Moving average of Reward for 100 episodes')\n",
    "    plt.savefig('moving_average_plot.png', dpi=1024)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_episode_actions(action_log, agent_name):\n",
    "    # plot episode actions through several episodes and its density\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=len(action_log), width_ratios=[3, 1], ncols=2, figsize=(15, 18)\n",
    "    )\n",
    "    plt.subplots_adjust(hspace=1.1)\n",
    "\n",
    "    fig.suptitle(f\"{agent_name}\\nactions in an episode \", fontsize=18, y=0.95)\n",
    "    for k_episode, ax, ax1 in zip(\n",
    "        action_log.keys(), axs.ravel()[::2], axs.ravel()[1::2]\n",
    "    ):\n",
    "        df = pd.DataFrame(action_log[k_episode])\n",
    "        df.index = np.arange(0, len(action_log[k_episode]), 1)\n",
    "        df.plot(ax=ax)\n",
    "        ax.set(xlabel='timestep', ylabel='action value')\n",
    "        df.plot(ax=ax1, kind='density')\n",
    "\n",
    "        ax.title.set_text(f'episode {k_episode}')\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "        ax1.title.set_text(f'episode {k_episode}')\n",
    "        ax1.get_legend().remove()\n",
    "\n",
    "    labels = ['Hip 1 Torque', 'Hip 2 Torque', 'Knee 1 Torque', 'Knee 2 Torque']\n",
    "    fig.legend(labels=labels, loc=\"upper right\")\n",
    "    plt.savefig('actions_plot.png', dpi=1024)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample_efficiency(*n_env, threshold=0.7):\n",
    "    # sample efficiency is generally measured as the number of required agent-environment\n",
    "    # interactions until a specified performance threshold is reached\n",
    "\n",
    "    sample_efficiency = [0] * len(n_env)\n",
    "    for i, env in enumerate(n_env):\n",
    "        try:\n",
    "            index_episode_threshold_is_reached = np.where(\n",
    "                np.array(env.return_queue).flatten() > int(threshold * 300)\n",
    "            )[0][0]\n",
    "            sample_efficiency[i] = sum(\n",
    "                np.array(env.length_queue).flatten()[\n",
    "                    :index_episode_threshold_is_reached\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            sample_efficiency[i] = 0\n",
    "\n",
    "    return sample_efficiency\n",
    "\n",
    "\n",
    "def robustness(agent, actor_weights, seed, n_episodes):\n",
    "    hardcore_env = create_env(seed, hardcore=True)\n",
    "    agent, hardcore_env, record_episode_actions_log = run_experiment(\n",
    "        hardcore_env, agent, n_episodes, seed, training=False\n",
    "    )\n",
    "    average_reward = np.mean(np.array(hardcore_env.return_queue).flatten())\n",
    "    return average_reward\n",
    "\n",
    "\n",
    "# sucess rate of the agent\n",
    "success_rate_ddpg = success_rate(ddpg_env)  # episode, episodic_reward\n",
    "\n",
    "# a list of reward moving average of 100 episodes\n",
    "R100_MA_ddpg = R100_MA(ddpg_env, total_n_episodes, ddpg_agent.name)\n",
    "\n",
    "# plot actions of various episodes\n",
    "plot_episode_actions(ddpg_record_episode_actions_log, ddpg_agent.name)\n",
    "\n",
    "# sample efficiency for ddpg  agent\n",
    "ddpg_sample_efficiency = sample_efficiency(ddpg_env, threshold=0.7)\n",
    "\n",
    "# measure robustness of ddpg agent by asking it to run for n episode in\n",
    "# hardcore enviroemnet\n",
    "# robustness_ddpg = robustness(ddpg_agent, seed=42, n_episodes=10)\n",
    "\n",
    "# plot various moving averages\n",
    "plot_R100_MA((R100_MA_ddpg, ddpg_env))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2115583511369930,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ai-msc-reinforcement-learning-bidepal-walker-assignment",
   "notebookOrigID": 2127183114195622,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
