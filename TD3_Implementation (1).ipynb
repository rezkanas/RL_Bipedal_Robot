{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc224fec-bcbb-4c22-a115-cb5b381f59bd",
     "showTitle": false,
     "title": ""
    },
    "id": "c4fzirPD6NCw"
   },
   "source": [
    "# Intro\n",
    "We will be training a reinforcement learning agent on the bipedal walker simulation as part of OpenAI's gym. We will need to interface correctly with this and create a TD3 baseline method, as well as a number of variants as specified in the report.\n",
    "\n",
    "Installs required for this workflow were based on a distributed paradigm and may not function correctly when ran locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb24398-e48e-4b5d-8753-861f754d02c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054939ba-02c7-4732-8e51-19f50ab2715f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pulkqsgz6m0E",
    "outputId": "d576554e-de25-4bbf-d02c-8e2cb675dcf5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Requirement already satisfied: gymnasium in /databricks/python3/lib/python3.8/site-packages (0.28.1)\r\n",
       "Requirement already satisfied: typing-extensions&gt;=4.3.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (4.5.0)\r\n",
       "Requirement already satisfied: farama-notifications&gt;=0.0.1 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (0.0.4)\r\n",
       "Requirement already satisfied: numpy&gt;=1.21.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.24.2)\r\n",
       "Requirement already satisfied: jax-jumpy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.0.0)\r\n",
       "Requirement already satisfied: importlib-metadata&gt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (6.5.1)\r\n",
       "Requirement already satisfied: cloudpickle&gt;=1.2.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.6.0)\r\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata&gt;=4.8.0-&gt;gymnasium) (3.4.1)\r\n",
       "<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Requirement already satisfied: gymnasium in /databricks/python3/lib/python3.8/site-packages (0.28.1)\r\nRequirement already satisfied: typing-extensions&gt;=4.3.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (4.5.0)\r\nRequirement already satisfied: farama-notifications&gt;=0.0.1 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (0.0.4)\r\nRequirement already satisfied: numpy&gt;=1.21.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.24.2)\r\nRequirement already satisfied: jax-jumpy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.0.0)\r\nRequirement already satisfied: importlib-metadata&gt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (6.5.1)\r\nRequirement already satisfied: cloudpickle&gt;=1.2.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium) (1.6.0)\r\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata&gt;=4.8.0-&gt;gymnasium) (3.4.1)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3601a8-0025-4ff0-9898-4fce03c31e5c",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2TJqlXN7M0G",
    "outputId": "ace72c13-e8e7-49bd-9f4c-3a99879a73f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Requirement already satisfied: gymnasium[box2d] in /databricks/python3/lib/python3.8/site-packages (0.28.1)\r\n",
       "Requirement already satisfied: typing-extensions&gt;=4.3.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.5.0)\r\n",
       "Requirement already satisfied: farama-notifications&gt;=0.0.1 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
       "Requirement already satisfied: numpy&gt;=1.21.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.24.2)\r\n",
       "Requirement already satisfied: jax-jumpy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.0.0)\r\n",
       "Requirement already satisfied: importlib-metadata&gt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (6.5.1)\r\n",
       "Requirement already satisfied: cloudpickle&gt;=1.2.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.6.0)\r\n",
       "Requirement already satisfied: box2d-py==2.3.5 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
       "Requirement already satisfied: pygame==2.1.3 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.1.3)\r\n",
       "Requirement already satisfied: swig==4.* in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.1.1)\r\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata&gt;=4.8.0-&gt;gymnasium[box2d]) (3.4.1)\r\n",
       "<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Requirement already satisfied: gymnasium[box2d] in /databricks/python3/lib/python3.8/site-packages (0.28.1)\r\nRequirement already satisfied: typing-extensions&gt;=4.3.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.5.0)\r\nRequirement already satisfied: farama-notifications&gt;=0.0.1 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (0.0.4)\r\nRequirement already satisfied: numpy&gt;=1.21.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.24.2)\r\nRequirement already satisfied: jax-jumpy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.0.0)\r\nRequirement already satisfied: importlib-metadata&gt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (6.5.1)\r\nRequirement already satisfied: cloudpickle&gt;=1.2.0 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.6.0)\r\nRequirement already satisfied: box2d-py==2.3.5 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.3.5)\r\nRequirement already satisfied: pygame==2.1.3 in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.1.3)\r\nRequirement already satisfied: swig==4.* in /databricks/python3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.1.1)\r\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata&gt;=4.8.0-&gt;gymnasium[box2d]) (3.4.1)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a537ee1-6f56-460f-8454-1d1bcd11b59a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Requirement already satisfied: ffmpeg in /databricks/python3/lib/python3.8/site-packages (1.4)\r\n",
       "<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Requirement already satisfied: ffmpeg in /databricks/python3/lib/python3.8/site-packages (1.4)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92818db3-ed53-4354-8700-43a67b716644",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Requirement already satisfied: moviepy in /databricks/python3/lib/python3.8/site-packages (1.0.3)\r\n",
       "Requirement already satisfied: proglog&lt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (0.1.10)\r\n",
       "Requirement already satisfied: imageio&lt;3.0,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (2.27.0)\r\n",
       "Requirement already satisfied: requests&lt;3.0,&gt;=2.8.1 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (2.25.1)\r\n",
       "Requirement already satisfied: numpy&gt;=1.17.3 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (1.24.2)\r\n",
       "Requirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (4.59.0)\r\n",
       "Requirement already satisfied: decorator&lt;5.0,&gt;=4.0.2 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (4.4.2)\r\n",
       "Requirement already satisfied: imageio-ffmpeg&gt;=0.2.0 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (0.4.8)\r\n",
       "Requirement already satisfied: pillow&gt;=8.3.2 in /databricks/python3/lib/python3.8/site-packages (from imageio&lt;3.0,&gt;=2.5-&gt;moviepy) (9.5.0)\r\n",
       "Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (1.25.11)\r\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (2020.12.5)\r\n",
       "Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (4.0.0)\r\n",
       "Requirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (2.10)\r\n",
       "<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Requirement already satisfied: moviepy in /databricks/python3/lib/python3.8/site-packages (1.0.3)\r\nRequirement already satisfied: proglog&lt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (0.1.10)\r\nRequirement already satisfied: imageio&lt;3.0,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (2.27.0)\r\nRequirement already satisfied: requests&lt;3.0,&gt;=2.8.1 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (2.25.1)\r\nRequirement already satisfied: numpy&gt;=1.17.3 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (1.24.2)\r\nRequirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (4.59.0)\r\nRequirement already satisfied: decorator&lt;5.0,&gt;=4.0.2 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (4.4.2)\r\nRequirement already satisfied: imageio-ffmpeg&gt;=0.2.0 in /databricks/python3/lib/python3.8/site-packages (from moviepy) (0.4.8)\r\nRequirement already satisfied: pillow&gt;=8.3.2 in /databricks/python3/lib/python3.8/site-packages (from imageio&lt;3.0,&gt;=2.5-&gt;moviepy) (9.5.0)\r\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (1.25.11)\r\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (2020.12.5)\r\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (4.0.0)\r\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy) (2.10)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 23.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42495c14-42e2-46c5-98f4-dfab5f37a219",
     "showTitle": false,
     "title": ""
    },
    "id": "is_J7LIxgM1H"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.2)\n",
       "  warnings.warn(f&#34;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of &#34;\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.2)\n  warnings.warn(f&#34;A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of &#34;\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "import datetime as dt\n",
    "import copy\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d077c0-2434-4cda-bc4e-a0d89bd83085",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "VDm-g1m-6JB4",
    "outputId": "141d236d-e400-4389-9807-622e1567cb74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[6]: </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[6]: </div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "data:image/png;base64,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",
       "datasetInfos": [],
       "metadata": {
        "imageDimensions": {
         "height": 252,
         "width": 366
        }
       },
       "removedWidgets": [],
       "type": "image"
      },
      "image/png": {
       "height": 252,
       "width": 366
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">&lt;matplotlib.image.AxesImage at 0x7f14b933ae80&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">&lt;matplotlib.image.AxesImage at 0x7f14b933ae80&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def time_now_string():\n",
    "    return dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  \n",
    "        pickle.dump(obj, outp)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as outp:  \n",
    "        object_ = pickle.load(outp)\n",
    "    return object_\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set seed across random and numpy libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return seed\n",
    "\n",
    "def create_env(seed):\n",
    "    # render_modes = [\"human\", \"rgb_array\", \"ansi\", \"rgb_array_list\"]\n",
    "    base_env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array_list\")\n",
    "    # env = RescaleAction(base_env, min_action=0, max_action=1)\n",
    "    # env = NormalizeObservation(env)\n",
    "\n",
    "    # Reset Environment\n",
    "    base_env.reset(seed=seed)\n",
    "\n",
    "    return base_env\n",
    "\n",
    "# Set Seed\n",
    "seed = set_seed(123)\n",
    "\n",
    "# Create Environment\n",
    "env = create_env(seed)\n",
    "\n",
    "# Render Environment\n",
    "plt.imshow(env.render()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74986c8-0767-41e4-8ee8-dc3c72e684f7",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RfMgDT9hfIA",
    "outputId": "f674f025-0445-45d7-be57-7eb4581cf309"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Bipedalwalker observe 24 parameters per state.\n",
       "To take a step in such an environment, 4 individual actions are taken to move hip and knee \n",
       "Max Value of individual action -&gt;  1.0, Min Value of individual action -&gt;  -1.0\n",
       "Reward range is  (-inf, inf)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Bipedalwalker observe 24 parameters per state.\nTo take a step in such an environment, 4 individual actions are taken to move hip and knee \nMax Value of individual action -&gt;  1.0, Min Value of individual action -&gt;  -1.0\nReward range is  (-inf, inf)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#environment information \n",
    "num_parameters_recorded_per_states = env.observation_space.shape[0]\n",
    "print(\"Bipedalwalker observe {} parameters per state.\".format(num_parameters_recorded_per_states))\n",
    "\n",
    "num_actions_per_step = env.action_space.shape[0]\n",
    "upper_limit = env.action_space.high[0]\n",
    "lower_limit = env.action_space.low[0]\n",
    "print(\"To take a step in such an environment, {} individual actions are taken to move hip and knee \".format(num_actions_per_step))\n",
    "print(\"Max Value of individual action ->  {}, Min Value of individual action ->  {}\".format(upper_limit, lower_limit))\n",
    "print(\"Reward range is  {}\".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ea4a0b-d81b-4c8f-b231-c3cee9be9561",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac217c19-7314-4581-9420-646c2c1f37e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f44089-dec6-4b03-9b7e-648920891829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    # Replay experience buffer, where experiences are stored to sample for model training\n",
    "    def __init__(self, max_size=200_000):\n",
    "        \"\"\"\n",
    "        max_size: Size of the buffer, how many transitions can be stored before deleting old transitions. \n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=self.max_size)\n",
    "        \n",
    "    def add(self, value_tuple):\n",
    "        \"\"\"\n",
    "        Add values to buffer.\n",
    "        The value tuple should be (state, next_state, action, reward, completed_flag)\n",
    "        \"\"\"\n",
    "        self.buffer.append(value_tuple)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample experiences from the memory buffer\n",
    "            batch_size: the amount of transitions to be randomly sampled at one time. \n",
    "        \"\"\"\n",
    "        ind = np.random.randint(0, len(self.buffer),  size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.buffer[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318e8012-f7db-44f1-bb03-56946c6c46a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ece4104-aebf-4c7c-8d3c-86eb2c766c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Actor(keras.Model):\n",
    "    \"\"\"Create actor network\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "\n",
    "        \"\"\"\n",
    "       state_dim: Observation environment dimensions, this will be used to determine network input size.\n",
    "       action_dim: Action dimensions, this will be used to determine network output.\n",
    "\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.inp_layer = Dense(\n",
    "            state_dim,\n",
    "        )\n",
    "        self.layer_1 = Dense(\n",
    "            400,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.layer_2 = Dense(\n",
    "            300,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.out_layer = Dense(\n",
    "            action_dim,\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "\n",
    "    def call(self, observation):\n",
    "        x = self.inp_layer(observation)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(keras.Model):\n",
    "    \"\"\"Creates two critic networks\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "    state_dim: The dimensions of the state the environment will produce.\n",
    "    action_dim: The dimensions of the actions the environment can take.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        # The First Critic NN\n",
    "        self.inp_layer_1 = Dense(\n",
    "            state_dim + action_dim,\n",
    "        )\n",
    "        self.layer_1_1 = Dense(\n",
    "            400,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.layer_2_1 = Dense(\n",
    "            300,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.out_layer_1 = Dense(\n",
    "            1,\n",
    "        )\n",
    "        # The Second Critic NN\n",
    "        self.inp_layer_2 = Dense(\n",
    "            state_dim + action_dim,\n",
    "        )\n",
    "        self.layer_1_2 = Dense(\n",
    "            400,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.layer_2_2 = Dense(\n",
    "            300,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.out_layer_2 = Dense(\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    def call(self, observation, action):\n",
    "        x0 = tf.concat([observation, action], 1)\n",
    "        # forward propagate the first critic network\n",
    "        x1 = self.inp_layer_1(x0)\n",
    "        x1 = self.layer_1_1(x1)\n",
    "        x1 = self.layer_2_1(x1)\n",
    "        x1 = self.out_layer_1(x1)\n",
    "        # forward propagate the second critic network\n",
    "        x2 = self.inp_layer_2(x0)\n",
    "        x2 = self.layer_1_2(x2)\n",
    "        x2 = self.layer_2_2(x2)\n",
    "        x2 = self.out_layer_2(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, observation, action):\n",
    "        # Forward propogate to first critic network only\n",
    "        x0 = tf.concat([observation, action], 1)\n",
    "        x1 = self.inp_layer_1(x0)\n",
    "        x1 = self.layer_1_1(x1)\n",
    "        x1 = self.layer_2_1(x1)\n",
    "        x1 = self.out_layer_1(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        noise_std=0.1,\n",
    "        noise_clip=0.5,\n",
    "        actor_train_interval=2,\n",
    "        actor_lr=4e-4,\n",
    "        critic_lr=4e-4,\n",
    "        loss_function=\"MSE\",\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "         state_dim: The dimensions of the state the environment will produce.\n",
    "                 This is the input for the Actor network and one of the inputs\n",
    "                 for the Critic network.\n",
    "         action_dim: The dimensions of the actions the environment can take.\n",
    "                 This is the output for the Actor network and one of the inputs\n",
    "                 for the Critic network.\n",
    "         max_action: The maximum value for each action dimension.\n",
    "         gamma: Future rewards discount factor.\n",
    "         tau: The factor that the target networks are soft updated, equivalent to how much.\n",
    "         noise_std: The scale factor to add noise to learning.\n",
    "         noise_clip: The maximum noise that can be added to actions during\n",
    "                 learning,\n",
    "         expl_noise: The scale factor for noise during action selection.\n",
    "         actor_train_interval: How often the actor network\n",
    "                 is trained and target networks are updated.\n",
    "         actor_lr: The learning rate used for gradient ascent of the Actor network.\n",
    "         critic_lr: The learning rate used for gradient descent of the Critic network\n",
    "         loss_function: can be set as MSE or Huber\n",
    "        \"\"\"\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        for t, e in zip(\n",
    "            self.actor_target.trainable_variables, self.actor.trainable_variables\n",
    "        ):\n",
    "            t.assign(e)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        for t, e in zip(\n",
    "            self.critic_target.trainable_variables, self.critic.trainable_variables\n",
    "        ):\n",
    "            t.assign(e)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "\n",
    "        if loss_function == \"MSE\":\n",
    "            self.critic_loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        elif loss_function == \"Huber\":\n",
    "            self.critic_loss_function = tf.keras.losses.Huber()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise_std = noise_std\n",
    "        self.noise_clip = noise_clip\n",
    "        self.actor_train_interval = actor_train_interval\n",
    "\n",
    "        self.train_it = 0\n",
    "\n",
    "    def select_action(self, state, noise: bool = False):\n",
    "        # Action selection by the actor_network.\n",
    "        state = state.reshape(1, -1)\n",
    "        action = self.actor.call(state)[0].numpy()\n",
    "        if noise:\n",
    "            noise = tf.random.normal(action.shape, mean=0, stddev=self.noise_std)\n",
    "            noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "            action = action + noise\n",
    "        action = tf.clip_by_value(action, -self.max_action, self.max_action)\n",
    "        return action\n",
    "\n",
    "    def explore(self, env, replay_buffer, explore_steps=10_000):\n",
    "        explore_counter = 0\n",
    "        done = True\n",
    "\n",
    "        while explore_counter < explore_steps:\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                done = False\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            done = terminal or truncated\n",
    "\n",
    "            replay_buffer.add((state, next_state, action, reward, terminal))\n",
    "\n",
    "            state = next_state\n",
    "            explore_counter += 1\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        # training of the Actor and Critic networks.\n",
    "        self.train_it += 1\n",
    "\n",
    "        # create a sample of transitions\n",
    "        (\n",
    "            batch_states,\n",
    "            batch_next_states,\n",
    "            batch_actions,\n",
    "            batch_rewards,\n",
    "            batch_completes,\n",
    "        ) = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # calculate a' and add noise\n",
    "        next_actions = self.actor_target.call(batch_next_states)\n",
    "\n",
    "        noise = tf.random.normal(next_actions.shape, mean=0, stddev=self.noise_std)\n",
    "        noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "        noisy_next_actions = tf.clip_by_value(\n",
    "            next_actions + noise, -self.max_action, self.max_action\n",
    "        )\n",
    "\n",
    "        # calculate the min(Q(s', a')) from the two critic target networks\n",
    "        target_q1, target_q2 = self.critic_target.call(\n",
    "            batch_next_states, noisy_next_actions\n",
    "        )\n",
    "        target_q = tf.minimum(target_q1, target_q2)\n",
    "\n",
    "        # calculate the target Q(s, a)\n",
    "        td_targets = tf.stop_gradient(\n",
    "            batch_rewards + (1 - batch_completes) * self.gamma * target_q\n",
    "        )\n",
    "\n",
    "        # Use gradient descent on the critic network\n",
    "        trainable_critic_variables = self.critic.trainable_variables\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_critic_variables)\n",
    "            model_q1, model_q2 = self.critic(batch_states, batch_actions)\n",
    "            critic_loss = self.critic_loss_function(\n",
    "                td_targets, model_q1\n",
    "            ) + self.critic_loss_function(td_targets, model_q2)\n",
    "        critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grads, trainable_critic_variables)\n",
    "        )\n",
    "\n",
    "        # Use gradient ascent on the actor network according to train_interval\n",
    "        if self.train_it % self.actor_train_interval == 0:\n",
    "            trainable_actor_variables = self.actor.trainable_variables\n",
    "\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(trainable_actor_variables)\n",
    "                actor_loss = -tf.reduce_mean(\n",
    "                    self.critic.Q1(batch_states, self.actor(batch_states))\n",
    "                )\n",
    "            actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n",
    "            self.actor_optimizer.apply_gradients(\n",
    "                zip(actor_grads, trainable_actor_variables)\n",
    "            )\n",
    "\n",
    "            # update the weights in the critic and actor target models\n",
    "            for t, e in zip(\n",
    "                self.critic_target.trainable_variables, self.critic.trainable_variables\n",
    "            ):\n",
    "                t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "\n",
    "            for t, e in zip(\n",
    "                self.actor_target.trainable_variables, self.actor.trainable_variables\n",
    "            ):\n",
    "                t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "\n",
    "    def save(self, video_render_path, agent_name, episode_idx):\n",
    "        # Save the weights of all the models.\n",
    "        self.actor.save_weights(\n",
    "            \"{}models/{}/actor_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "        self.actor_target.save_weights(\n",
    "            \"{}models/{}/actor_target_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "\n",
    "        self.critic.save_weights(\n",
    "            \"{}models/{}/critic_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "        self.critic_target.save_weights(\n",
    "            \"{}models/{}/critic_target_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a273a8d4-9bf3-47f7-948d-6e3266c8db7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Store Details\n",
    "Run a test for 100 episodes to check these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaed2bb-c5ad-4e32-9bca-5a5d460c4363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">video_render_path = &#39;/dbfs/DataScience/ch_test_dir/RL/td3_baseline_1000_3/&#39;\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">video_render_path = &#39;/dbfs/DataScience/ch_test_dir/RL/td3_baseline_1000_3/&#39;\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File System Parameters\n",
    "ROOT = '.' \n",
    "\n",
    "agent_name = 'td3_baseline_1000_3'\n",
    "\n",
    "video_render_path = f'{ROOT}/{agent_name}/'\n",
    "\n",
    "print(f'{video_render_path = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7d652e-6a59-4033-aef0-ed6f97a2ee61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">22/04/2023 13:06:01 Starting Exploration\n",
       "22/04/2023 13:11:28 Completed Exploration Steps: 50000\n",
       "Saving Details.\n",
       "Episode: 10. Eval Metric: -101.81\n",
       "Episode: 20. Eval Metric: -121.38\n",
       "Episode: 30. Eval Metric: -126.81\n",
       "Episode: 40. Eval Metric: -125.46\n",
       "Episode: 50. Eval Metric: -124.03\n",
       "Episode: 60. Eval Metric: -122.11\n",
       "Episode: 70. Eval Metric: -120.26\n",
       "Episode: 80. Eval Metric: -118.21\n",
       "Episode: 90. Eval Metric: -116.85\n",
       "22/04/2023 13:24:30 Episode: 100, Total Timesteps: 16,168, Episode Timesteps: 45, Episode Reward: -105.20\n",
       "Episode: 100. Eval Metric: -115.59\n",
       "Saving Details.\n",
       "Episode: 110. Eval Metric: -115.96\n",
       "Episode: 120. Eval Metric: -112.26\n",
       "Episode: 130. Eval Metric: -108.75\n",
       "Episode: 140. Eval Metric: -108.02\n",
       "Episode: 150. Eval Metric: -106.84\n",
       "Episode: 160. Eval Metric: -105.85\n",
       "Episode: 170. Eval Metric: -105.16\n",
       "Episode: 180. Eval Metric: -105.67\n",
       "Episode: 190. Eval Metric: -106.00\n",
       "22/04/2023 13:32:22 Episode: 200, Total Timesteps: 25,942, Episode Timesteps: 161, Episode Reward: -135.95\n",
       "Episode: 200. Eval Metric: -107.05\n",
       "Saving Details.\n",
       "Episode: 210. Eval Metric: -109.28\n",
       "Episode: 220. Eval Metric: -109.72\n",
       "Episode: 230. Eval Metric: -110.68\n",
       "Episode: 240. Eval Metric: -111.58\n",
       "Episode: 250. Eval Metric: -112.88\n",
       "Episode: 260. Eval Metric: -114.76\n",
       "Episode: 270. Eval Metric: -116.82\n",
       "Episode: 280. Eval Metric: -118.25\n",
       "Episode: 290. Eval Metric: -119.95\n",
       "22/04/2023 14:10:43 Episode: 300, Total Timesteps: 73,131, Episode Timesteps: 1,600, Episode Reward: -65.77\n",
       "Episode: 300. Eval Metric: -118.23\n",
       "Saving Details.\n",
       "Episode: 310. Eval Metric: -113.62\n",
       "Episode: 320. Eval Metric: -111.15\n",
       "Episode: 330. Eval Metric: -106.93\n",
       "Episode: 340. Eval Metric: -103.03\n",
       "Episode: 350. Eval Metric: -96.19\n",
       "Episode: 360. Eval Metric: -83.57\n",
       "Episode: 370. Eval Metric: -73.32\n",
       "Episode: 380. Eval Metric: -67.48\n",
       "Episode: 390. Eval Metric: -61.53\n",
       "22/04/2023 16:02:04 Episode: 400, Total Timesteps: 203,049, Episode Timesteps: 1,600, Episode Reward: 98.73\n",
       "Episode: 400. Eval Metric: -51.95\n",
       "Saving Details.\n",
       "Episode: 410. Eval Metric: -29.37\n",
       "Episode: 420. Eval Metric: 1.11\n",
       "Episode: 430. Eval Metric: 29.15\n",
       "Episode: 440. Eval Metric: 59.05\n",
       "Episode: 450. Eval Metric: 90.55\n",
       "Episode: 460. Eval Metric: 116.79\n",
       "Episode: 470. Eval Metric: 145.75\n",
       "Episode: 480. Eval Metric: 175.51\n",
       "Episode: 490. Eval Metric: 193.70\n",
       "22/04/2023 17:49:39 Episode: 500, Total Timesteps: 318,009, Episode Timesteps: 1,064, Episode Reward: 272.86\n",
       "Episode: 500. Eval Metric: 221.12\n",
       "Saving Details.\n",
       "Episode: 510. Eval Metric: 234.07\n",
       "Episode: 520. Eval Metric: 239.38\n",
       "Episode: 530. Eval Metric: 245.82\n",
       "Episode: 540. Eval Metric: 252.10\n",
       "Episode: 550. Eval Metric: 251.99\n",
       "Episode: 560. Eval Metric: 253.41\n",
       "Episode: 570. Eval Metric: 246.98\n",
       "Episode: 580. Eval Metric: 248.83\n",
       "Episode: 590. Eval Metric: 263.45\n",
       "22/04/2023 19:19:02 Episode: 600, Total Timesteps: 413,370, Episode Timesteps: 898, Episode Reward: 285.01\n",
       "Episode: 600. Eval Metric: 265.11\n",
       "Saving Details.\n",
       "Episode: 610. Eval Metric: 257.71\n",
       "Episode: 620. Eval Metric: 252.02\n",
       "Episode: 630. Eval Metric: 247.72\n",
       "Episode: 640. Eval Metric: 243.14\n",
       "Episode: 650. Eval Metric: 246.48\n",
       "Episode: 660. Eval Metric: 248.43\n",
       "Episode: 670. Eval Metric: 254.89\n",
       "Episode: 680. Eval Metric: 256.44\n",
       "Episode: 690. Eval Metric: 260.36\n",
       "22/04/2023 20:30:34 Episode: 700, Total Timesteps: 490,130, Episode Timesteps: 799, Episode Reward: 300.39\n",
       "Episode: 700. Eval Metric: 261.89\n",
       "Saving Details.\n",
       "Episode: 710. Eval Metric: 272.24\n",
       "Episode: 720. Eval Metric: 280.97\n",
       "Episode: 730. Eval Metric: 288.23\n",
       "Episode: 740. Eval Metric: 295.52\n",
       "Episode: 750. Eval Metric: 296.37\n",
       "Episode: 760. Eval Metric: 292.49\n",
       "Episode: 770. Eval Metric: 296.20\n",
       "Episode: 780. Eval Metric: 300.23\n",
       "Episode: 790. Eval Metric: 300.83\n",
       "22/04/2023 21:40:48 Episode: 800, Total Timesteps: 564,933, Episode Timesteps: 742, Episode Reward: 305.12\n",
       "Episode: 800. Eval Metric: 300.98\n",
       "Saving Details.\n",
       "Episode: 810. Eval Metric: 301.34\n",
       "Episode: 820. Eval Metric: 301.59\n",
       "Episode: 830. Eval Metric: 301.78\n",
       "Episode: 840. Eval Metric: 300.97\n",
       "Episode: 850. Eval Metric: 301.22\n",
       "Episode: 860. Eval Metric: 305.79\n",
       "Episode: 870. Eval Metric: 303.18\n",
       "Episode: 880. Eval Metric: 297.76\n",
       "Episode: 890. Eval Metric: 297.01\n",
       "22/04/2023 22:49:01 Episode: 900, Total Timesteps: 638,031, Episode Timesteps: 791, Episode Reward: 304.17\n",
       "Episode: 900. Eval Metric: 296.98\n",
       "Saving Details.\n",
       "Episode: 910. Eval Metric: 294.04\n",
       "Episode: 920. Eval Metric: 289.95\n",
       "Episode: 930. Eval Metric: 289.95\n",
       "Episode: 940. Eval Metric: 290.83\n",
       "Episode: 950. Eval Metric: 288.13\n",
       "Episode: 960. Eval Metric: 288.11\n",
       "Episode: 970. Eval Metric: 290.82\n",
       "Episode: 980. Eval Metric: 296.19\n",
       "Episode: 990. Eval Metric: 297.16\n",
       "Saving Details.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">22/04/2023 13:06:01 Starting Exploration\n22/04/2023 13:11:28 Completed Exploration Steps: 50000\nSaving Details.\nEpisode: 10. Eval Metric: -101.81\nEpisode: 20. Eval Metric: -121.38\nEpisode: 30. Eval Metric: -126.81\nEpisode: 40. Eval Metric: -125.46\nEpisode: 50. Eval Metric: -124.03\nEpisode: 60. Eval Metric: -122.11\nEpisode: 70. Eval Metric: -120.26\nEpisode: 80. Eval Metric: -118.21\nEpisode: 90. Eval Metric: -116.85\n22/04/2023 13:24:30 Episode: 100, Total Timesteps: 16,168, Episode Timesteps: 45, Episode Reward: -105.20\nEpisode: 100. Eval Metric: -115.59\nSaving Details.\nEpisode: 110. Eval Metric: -115.96\nEpisode: 120. Eval Metric: -112.26\nEpisode: 130. Eval Metric: -108.75\nEpisode: 140. Eval Metric: -108.02\nEpisode: 150. Eval Metric: -106.84\nEpisode: 160. Eval Metric: -105.85\nEpisode: 170. Eval Metric: -105.16\nEpisode: 180. Eval Metric: -105.67\nEpisode: 190. Eval Metric: -106.00\n22/04/2023 13:32:22 Episode: 200, Total Timesteps: 25,942, Episode Timesteps: 161, Episode Reward: -135.95\nEpisode: 200. Eval Metric: -107.05\nSaving Details.\nEpisode: 210. Eval Metric: -109.28\nEpisode: 220. Eval Metric: -109.72\nEpisode: 230. Eval Metric: -110.68\nEpisode: 240. Eval Metric: -111.58\nEpisode: 250. Eval Metric: -112.88\nEpisode: 260. Eval Metric: -114.76\nEpisode: 270. Eval Metric: -116.82\nEpisode: 280. Eval Metric: -118.25\nEpisode: 290. Eval Metric: -119.95\n22/04/2023 14:10:43 Episode: 300, Total Timesteps: 73,131, Episode Timesteps: 1,600, Episode Reward: -65.77\nEpisode: 300. Eval Metric: -118.23\nSaving Details.\nEpisode: 310. Eval Metric: -113.62\nEpisode: 320. Eval Metric: -111.15\nEpisode: 330. Eval Metric: -106.93\nEpisode: 340. Eval Metric: -103.03\nEpisode: 350. Eval Metric: -96.19\nEpisode: 360. Eval Metric: -83.57\nEpisode: 370. Eval Metric: -73.32\nEpisode: 380. Eval Metric: -67.48\nEpisode: 390. Eval Metric: -61.53\n22/04/2023 16:02:04 Episode: 400, Total Timesteps: 203,049, Episode Timesteps: 1,600, Episode Reward: 98.73\nEpisode: 400. Eval Metric: -51.95\nSaving Details.\nEpisode: 410. Eval Metric: -29.37\nEpisode: 420. Eval Metric: 1.11\nEpisode: 430. Eval Metric: 29.15\nEpisode: 440. Eval Metric: 59.05\nEpisode: 450. Eval Metric: 90.55\nEpisode: 460. Eval Metric: 116.79\nEpisode: 470. Eval Metric: 145.75\nEpisode: 480. Eval Metric: 175.51\nEpisode: 490. Eval Metric: 193.70\n22/04/2023 17:49:39 Episode: 500, Total Timesteps: 318,009, Episode Timesteps: 1,064, Episode Reward: 272.86\nEpisode: 500. Eval Metric: 221.12\nSaving Details.\nEpisode: 510. Eval Metric: 234.07\nEpisode: 520. Eval Metric: 239.38\nEpisode: 530. Eval Metric: 245.82\nEpisode: 540. Eval Metric: 252.10\nEpisode: 550. Eval Metric: 251.99\nEpisode: 560. Eval Metric: 253.41\nEpisode: 570. Eval Metric: 246.98\nEpisode: 580. Eval Metric: 248.83\nEpisode: 590. Eval Metric: 263.45\n22/04/2023 19:19:02 Episode: 600, Total Timesteps: 413,370, Episode Timesteps: 898, Episode Reward: 285.01\nEpisode: 600. Eval Metric: 265.11\nSaving Details.\nEpisode: 610. Eval Metric: 257.71\nEpisode: 620. Eval Metric: 252.02\nEpisode: 630. Eval Metric: 247.72\nEpisode: 640. Eval Metric: 243.14\nEpisode: 650. Eval Metric: 246.48\nEpisode: 660. Eval Metric: 248.43\nEpisode: 670. Eval Metric: 254.89\nEpisode: 680. Eval Metric: 256.44\nEpisode: 690. Eval Metric: 260.36\n22/04/2023 20:30:34 Episode: 700, Total Timesteps: 490,130, Episode Timesteps: 799, Episode Reward: 300.39\nEpisode: 700. Eval Metric: 261.89\nSaving Details.\nEpisode: 710. Eval Metric: 272.24\nEpisode: 720. Eval Metric: 280.97\nEpisode: 730. Eval Metric: 288.23\nEpisode: 740. Eval Metric: 295.52\nEpisode: 750. Eval Metric: 296.37\nEpisode: 760. Eval Metric: 292.49\nEpisode: 770. Eval Metric: 296.20\nEpisode: 780. Eval Metric: 300.23\nEpisode: 790. Eval Metric: 300.83\n22/04/2023 21:40:48 Episode: 800, Total Timesteps: 564,933, Episode Timesteps: 742, Episode Reward: 305.12\nEpisode: 800. Eval Metric: 300.98\nSaving Details.\nEpisode: 810. Eval Metric: 301.34\nEpisode: 820. Eval Metric: 301.59\nEpisode: 830. Eval Metric: 301.78\nEpisode: 840. Eval Metric: 300.97\nEpisode: 850. Eval Metric: 301.22\nEpisode: 860. Eval Metric: 305.79\nEpisode: 870. Eval Metric: 303.18\nEpisode: 880. Eval Metric: 297.76\nEpisode: 890. Eval Metric: 297.01\n22/04/2023 22:49:01 Episode: 900, Total Timesteps: 638,031, Episode Timesteps: 791, Episode Reward: 304.17\nEpisode: 900. Eval Metric: 296.98\nSaving Details.\nEpisode: 910. Eval Metric: 294.04\nEpisode: 920. Eval Metric: 289.95\nEpisode: 930. Eval Metric: 289.95\nEpisode: 940. Eval Metric: 290.83\nEpisode: 950. Eval Metric: 288.13\nEpisode: 960. Eval Metric: 288.11\nEpisode: 970. Eval Metric: 290.82\nEpisode: 980. Eval Metric: 296.19\nEpisode: 990. Eval Metric: 297.16\nSaving Details.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Start Time\n",
    "start_time = time_now_string()\n",
    "\n",
    "# initialise the environment\n",
    "env = create_env(seed=123)\n",
    "#record statistics \n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size= 1_000_000)\n",
    "\n",
    "# Get action & state details\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# initialise the replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=200_000)\n",
    "# initialise the policy\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "\n",
    "# Set Reward Checkpoints to store details\n",
    "performance_checkpoints = [-150.0, 0.0, 50.0, 100.0, 200.0, 250.0, 300.0, 500.0]\n",
    "\n",
    "max_episodes = 1000\n",
    "total_timesteps = 0\n",
    "eval_freq = 10\n",
    "episode_num = 0\n",
    "episode_reward = 0\n",
    "done = True\n",
    "\n",
    "# Store actions for analysis\n",
    "record_episode_actions_log = defaultdict(list)\n",
    "\n",
    "running_average = []\n",
    "\n",
    "# Run Exploration\n",
    "print(f'{time_now_string()} Starting Exploration')\n",
    "\n",
    "explore_steps = 50_000\n",
    "\n",
    "policy.explore(env, replay_buffer, explore_steps)\n",
    "\n",
    "print(f'{time_now_string()} Completed Exploration Steps: {explore_steps}')\n",
    "\n",
    "while episode_num < max_episodes:\n",
    "\n",
    "\tif done:\n",
    "\n",
    "    # collect actions of some episode to use them to analyze algorithm performance\n",
    "\t\trecord_episode_actions = False\n",
    "\t\tif episode_num % 10 == 0:\n",
    "\t\t\trecord_episode_actions = True\n",
    "\n",
    "\t\t# print the results at the end of the episode\n",
    "\t\tif total_timesteps != 0 and episode_num % 100 == 0:\n",
    "\t\t\tprint('{} Episode: {}, Total Timesteps: {:,}, Episode Timesteps: {:,}, Episode Reward: {:.2f}'.format(\n",
    "\t\t\t\ttime_now_string(),\n",
    "\t\t\t\tepisode_num,\n",
    "\t\t\t\ttotal_timesteps,\n",
    "\t\t\t\tepisode_timesteps,\n",
    "\t\t\t\tepisode_reward\n",
    "\t\t\t\t)\n",
    "        )\n",
    "\n",
    "\t\tif episode_num % eval_freq == 0 and episode_num > 0: \n",
    "\t\t\tmoving_average_score = np.mean(running_average[-100:])\n",
    "\t\t\tprint(f'Episode: {episode_num}. Eval Metric: {moving_average_score:.2f}')\n",
    "\n",
    "\t  # save the model    \n",
    "\t\tif episode_num % 100 == 0:\n",
    "\t\t\tprint('Saving Details.')\n",
    "\t\t\tpolicy.save(video_render_path, agent_name, episode_num)\n",
    "\n",
    "\t\t\t# Continually save down all results every 100 episodes\n",
    "\t\t\tep_rewards = np.concatenate(list(env.return_queue))\n",
    "\t\t\tep_duration = np.concatenate(list(env.length_queue))\n",
    "\t\t\tep_actions = record_episode_actions_log\n",
    "\t\t\t\n",
    "\t\t\tpolicy.save(video_render_path, agent_name, 'final_policy')\n",
    "\t\t\t\n",
    "\t\t\tsave_object(ep_rewards, video_render_path+f\"ep_rewards_{episode_num}\")\n",
    "\t\t\tsave_object(ep_duration, video_render_path+f\"ep_duration_{episode_num}\")\n",
    "\t\t\tsave_object(ep_actions, video_render_path+f\"ep_actions_{episode_num}\")\n",
    "\n",
    "\n",
    "\t\tstate, _ = env.reset()\n",
    "\n",
    "\t\tdone = False\n",
    "\t\trunning_average.append(episode_reward)\n",
    "\t\n",
    "\t\tepisode_reward = 0\n",
    "\t\tepisode_timesteps = 0\n",
    "\t\tepisode_num += 1\n",
    "\n",
    "\n",
    "\telse: # select an action from the actor network with noise\n",
    "\t\taction = policy.select_action(state, noise=True).numpy()\n",
    "\t\tif record_episode_actions:\n",
    "\t\t\trecord_episode_actions_log[episode_num].append(action)\n",
    "\n",
    "\t# the agent plays the action\n",
    "\t\tnext_state, reward, terminal, truncated, info = env.step(action)\n",
    " \n",
    "\t\tdone = terminal or truncated\n",
    "\n",
    "\t# add to the total episode reward\n",
    "\t\tepisode_reward += reward\n",
    "\n",
    "\t# add to the memory buffer\n",
    "\t\treplay_buffer.add((state, next_state, action, reward, terminal))\n",
    "\n",
    "\t# update the state, episode timestep and total timestep\n",
    "\t\tstate = next_state\n",
    "\t\tepisode_timesteps += 1\n",
    "\t\ttotal_timesteps += 1\n",
    "\n",
    "\t# train after the first episode\n",
    "\t\tif total_timesteps > 0:\n",
    "\t\t\tpolicy.train(replay_buffer)\n",
    "    \n",
    "print('Saving Details.')\n",
    "policy.save(video_render_path, agent_name, episode_num)\n",
    "\n",
    "# Continually save down all results every 100 episodes\n",
    "ep_rewards = np.concatenate(list(env.return_queue))\n",
    "ep_duration = np.concatenate(list(env.length_queue))\n",
    "ep_actions = record_episode_actions_log\n",
    "\n",
    "policy.save(video_render_path, agent_name, 'final_policy')\n",
    "\n",
    "save_object(ep_rewards, video_render_path+f\"ep_rewards_{episode_num}\")\n",
    "save_object(ep_duration, video_render_path+f\"ep_duration_{episode_num}\")\n",
    "save_object(ep_actions, video_render_path+f\"ep_actions_{episode_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritised Replay Experience Agent\n",
    "\n",
    "Updates are required for the replay buffer and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    # The memory to store transitions as the agent plays the environment\n",
    "    def __init__(self, max_size=200_000, td_error_prioritization: bool = False):\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=self.max_size)\n",
    "        self.td_loss = deque(maxlen=self.max_size)\n",
    "        self.td_error_prioritization = td_error_prioritization\n",
    "        \n",
    "    def add(self, value_tuple, td_error=0):\n",
    "\n",
    "        self.buffer.append(value_tuple)\n",
    "        if self.td_error_prioritization:\n",
    "            self.td_loss.append(td_error)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        if self.td_error_prioritization:\n",
    "            weights = self.td_loss / sum(self.td_loss)  \n",
    "            batch_indices = np.random.choice(range(len(self.buffer)), size=self.batch_size, p = weights)\n",
    "\n",
    "        else:\n",
    "            ind = np.random.randint(0, len(self.buffer),  size=batch_size)\n",
    "\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.buffer[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        noise_std=0.1,\n",
    "        noise_clip=0.5,\n",
    "        actor_train_interval=2,\n",
    "        actor_lr=4e-4,\n",
    "        critic_lr=4e-4,\n",
    "        loss_function=\"MSE\",\n",
    "        tdl_priority=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "                state_dim: The dimensions of the state the environment will produce.\n",
    "                        This is the input for the Actor network and one of the inputs\n",
    "                        for the Critic network.\n",
    "                action_dim: The dimensions of the actions the environment can take.\n",
    "                        This is the output for the Actor network and one of the inputs\n",
    "                        for the Critic network.\n",
    "                max_action: The maximum value for each action dimension.\n",
    "                gamma: Future rewards discount factor.\n",
    "                tau: The factor that the target networks are soft updated, equivalent to how much.\n",
    "                noise_std: The scale factor to add noise to learning.\n",
    "                noise_clip: The maximum noise that can be added to actions during\n",
    "                        learning,\n",
    "                expl_noise: The scale factor for noise during action selection.\n",
    "                actor_train_interval: How often the actor network\n",
    "                        is trained and target networks are updated.\n",
    "                actor_lr: The learning rate used for gradient ascent of the Actor network.\n",
    "                critic_lr: The learning rate used for gradient descent of the Critic network\n",
    "                loss_function: can be set as MSE or Huber\n",
    "        \"\"\"\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        for t, e in zip(\n",
    "            self.actor_target.trainable_variables, self.actor.trainable_variables\n",
    "        ):\n",
    "            t.assign(e) \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        for t, e in zip(\n",
    "            self.critic_target.trainable_variables, self.critic.trainable_variables\n",
    "        ):\n",
    "            t.assign(e)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "\n",
    "        if loss_function == \"MSE\":\n",
    "            self.critic_loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        elif loss_function == \"Huber\":\n",
    "            self.critic_loss_function = tf.keras.losses.Huber()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise_std = noise_std\n",
    "        self.noise_clip = noise_clip\n",
    "        self.actor_train_interval = actor_train_interval\n",
    "        self.tdl_priority = tdl_priority\n",
    "\n",
    "        self.train_it = 0\n",
    "\n",
    "    def select_action(self, state, noise: bool = False):\n",
    "        # Action selection by the actor_network.\n",
    "        state = state.reshape(1, -1)\n",
    "        action = self.actor.call(state)[0].numpy()\n",
    "        if noise:\n",
    "            noise = tf.random.normal(action.shape, mean=0, stddev=self.noise_std)\n",
    "            noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "            action = action + noise\n",
    "        action = tf.clip_by_value(action, -self.max_action, self.max_action)\n",
    "        return action\n",
    "\n",
    "    def explore(self, env, replay_buffer, explore_steps=10_000, td_error_prioritization: bool = False):\n",
    "        explore_counter = 0\n",
    "        done = True\n",
    "\n",
    "        \n",
    "\n",
    "        while explore_counter < explore_steps:\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                done = False\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            done = terminal or truncated\n",
    "\n",
    "            if td_error_prioritization:\n",
    "                replay_buffer.add((state, next_state, action, reward, terminal), td_error=1)\n",
    "\n",
    "            else:\n",
    "                replay_buffer.add((state, next_state, action, reward, terminal))\n",
    "\n",
    "            state = next_state\n",
    "            explore_counter += 1\n",
    "\n",
    "    def train(self, observation, replay_buffer, batch_size=100):\n",
    "        # training of the Actor and Critic networks.\n",
    "        self.train_it += 1\n",
    "\n",
    "        if self.tdl_priority:\n",
    "          # batch_rewards + (1 - batch_completes) * self.gamma * target_q\n",
    "\n",
    "            prev_state, state, action, reward, complete_flag = observation\n",
    "            next_action = self.select_action(state, noise=True).numpy()\n",
    "            next_state_prediction = min(self.critic_target.call(\n",
    "            [state], [next_action]\n",
    "            )\n",
    "            )\n",
    "            y = reward + (1 - complete_flag) * self.gamma * next_state_prediction\n",
    "\n",
    "            y_chapeau = self.critic.call([prev_state], [action])\n",
    "            td_loss = abs(y - y_chapeau).numpy()\n",
    "\n",
    "        else:\n",
    "          \n",
    "          td_loss = 0\n",
    "\n",
    "        #store transition in buffer\n",
    "        replay_buffer.add(observation, td_loss)\n",
    "\n",
    "        # create a sample of transitions\n",
    "        (\n",
    "            batch_states,\n",
    "            batch_next_states,\n",
    "            batch_actions,\n",
    "            batch_rewards,\n",
    "            batch_completes,\n",
    "        ) = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # calculate a' and add noise\n",
    "        next_actions = self.actor_target.call(batch_next_states)\n",
    "\n",
    "        noise = tf.random.normal(next_actions.shape, mean=0, stddev=self.noise_std)\n",
    "        noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "        noisy_next_actions = tf.clip_by_value(\n",
    "            next_actions + noise, -self.max_action, self.max_action\n",
    "        )\n",
    "\n",
    "        # calculate the min(Q(s', a')) from the two critic target networks\n",
    "        target_q1, target_q2 = self.critic_target.call(\n",
    "            batch_next_states, noisy_next_actions\n",
    "        )\n",
    "        target_q = tf.minimum(target_q1, target_q2)\n",
    "\n",
    "        # calculate the target Q(s, a)\n",
    "        td_targets = tf.stop_gradient(\n",
    "            batch_rewards + (1 - batch_completes) * self.gamma * target_q\n",
    "        )\n",
    "\n",
    "        # Use gradient descent on the critic network\n",
    "        trainable_critic_variables = self.critic.trainable_variables\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_critic_variables)\n",
    "            model_q1, model_q2 = self.critic(batch_states, batch_actions)\n",
    "            critic_loss = self.critic_loss_function(\n",
    "                td_targets, model_q1\n",
    "            ) + self.critic_loss_function(td_targets, model_q2)\n",
    "        critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grads, trainable_critic_variables)\n",
    "        )\n",
    "\n",
    "        # Use gradient ascent on the actor network according to train_interval\n",
    "        if self.train_it % self.actor_train_interval == 0:\n",
    "            trainable_actor_variables = self.actor.trainable_variables\n",
    "\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(trainable_actor_variables)\n",
    "                actor_loss = -tf.reduce_mean(\n",
    "                    self.critic.Q1(batch_states, self.actor(batch_states))\n",
    "                )\n",
    "            actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n",
    "            self.actor_optimizer.apply_gradients(\n",
    "                zip(actor_grads, trainable_actor_variables)\n",
    "            )\n",
    "\n",
    "            # update the weights in the critic and actor target models\n",
    "            for t, e in zip(\n",
    "                self.critic_target.trainable_variables, self.critic.trainable_variables\n",
    "            ):\n",
    "                t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "\n",
    "            for t, e in zip(\n",
    "                self.actor_target.trainable_variables, self.actor.trainable_variables\n",
    "            ):\n",
    "                t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "\n",
    "    def save(self, video_render_path, agent_name, episode_idx):\n",
    "        # Save the weights of all the models.\n",
    "        self.actor.save_weights(\n",
    "            \"{}models/{}/actor_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "        self.actor_target.save_weights(\n",
    "            \"{}models/{}/actor_target_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "\n",
    "        self.critic.save_weights(\n",
    "            \"{}models/{}/critic_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )\n",
    "        self.critic_target.save_weights(\n",
    "            \"{}models/{}/critic_target_ep{}\".format(video_render_path, agent_name, episode_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = 'td3_priority_1000'\n",
    "\n",
    "video_render_path = f'{ROOT}/{agent_name}/'\n",
    "\n",
    "print(f'{video_render_path = }')\n",
    "\n",
    "start_time = time_now_string()\n",
    "\n",
    "# initialise the environment\n",
    "env = create_env(seed=123)\n",
    "#record statistics \n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size= 1_000_000)\n",
    "\n",
    "# Get action & state details\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# initialise the replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=200_000)\n",
    "# initialise the policy\n",
    "policy = TD3(state_dim, action_dim, max_action, tdl_priority=True)\n",
    "\n",
    "performance_checkpoints = [-150.0, 0.0, 50.0, 100.0, 200.0, 250.0, 300.0, 500.0]\n",
    "\n",
    "max_episodes = 1000\n",
    "total_timesteps = 0\n",
    "eval_freq = 10\n",
    "save_freq = 1e5\n",
    "episode_num = 0\n",
    "episode_reward = 0\n",
    "done = True\n",
    "\n",
    "# Store actions for analysis\n",
    "record_episode_actions_log = defaultdict(list)\n",
    "\n",
    "running_average = []\n",
    "\n",
    "# Run Exploration\n",
    "print(f'{time_now_string()} Starting Exploration')\n",
    "\n",
    "explore_steps = 50_000\n",
    "\n",
    "policy.explore(env, replay_buffer, explore_steps)\n",
    "\n",
    "print(f'{time_now_string()} Completed Exploration Steps: {explore_steps}')\n",
    "\n",
    "while episode_num < max_episodes:\n",
    "\n",
    "\tif done:\n",
    "\n",
    "    # collect actions of some episode to use them to analyze algorithm performance\n",
    "\t\trecord_episode_actions = False\n",
    "\t\tif episode_num % 10 == 0:\n",
    "\t\t\trecord_episode_actions = True\n",
    "\n",
    "\t\t# print the results at the end of the episode\n",
    "\t\tif total_timesteps != 0 and episode_num % 100 == 0:\n",
    "\t\t\tprint('{} Episode: {}, Total Timesteps: {:,}, Episode Timesteps: {:,}, Episode Reward: {:.2f}'.format(\n",
    "\t\t\t\ttime_now_string(),\n",
    "\t\t\t\tepisode_num,\n",
    "\t\t\t\ttotal_timesteps,\n",
    "\t\t\t\tepisode_timesteps,\n",
    "\t\t\t\tepisode_reward\n",
    "\t\t\t\t)\n",
    "        )\n",
    "\n",
    "\t\tif episode_num % eval_freq == 0 and episode_num > 0: \n",
    "\t\t\tmoving_average_score = np.mean(running_average[-100:])\n",
    "\t\t\tprint(f'Episode: {episode_num}. Eval Metric: {moving_average_score:.2f}')\n",
    "\n",
    "\t\tif episode_reward > min(performance_checkpoints) or episode_num % 100 == 0:\n",
    "\t\t\t\tcheckpoint = min(performance_checkpoints)\n",
    "\n",
    "\t\t\t\tprint(f'Storing Video for checkpoint: {checkpoint}')\n",
    "\n",
    "\t\t\t\t# Store Video\n",
    "\t\t\t\tsave_object(env, video_render_path+'_'+str(episode_num))\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Update list, remove smallest element\n",
    "\t\t\t\tif episode_reward > min(performance_checkpoints):\n",
    "\t\t\t\t\tperformance_checkpoints.remove(checkpoint)\n",
    "\n",
    "\t  # save the model    \n",
    "\t\tif episode_num % 100 == 0:\n",
    "\t\t\tprint('Saving Details.')\n",
    "\t\t\tpolicy.save(video_render_path, agent_name, episode_num)\n",
    "\n",
    "\t\t\t# Continually save down all results every 100 episodes\n",
    "\t\t\tep_rewards = np.concatenate(list(env.return_queue))\n",
    "\t\t\tep_duration = np.concatenate(list(env.length_queue))\n",
    "\t\t\tep_actions = record_episode_actions_log\n",
    "\t\t\t\n",
    "\t\t\tpolicy.save(video_render_path, agent_name, 'final_policy')\n",
    "\t\t\t\n",
    "\t\t\tsave_object(ep_rewards, video_render_path+f\"ep_rewards_{episode_num}\")\n",
    "\t\t\tsave_object(ep_duration, video_render_path+f\"ep_duration_{episode_num}\")\n",
    "\t\t\tsave_object(ep_actions, video_render_path+f\"ep_actions_{episode_num}\")\n",
    "\n",
    "\n",
    "\t\tstate, _ = env.reset()\n",
    "\n",
    "\t\tdone = False\n",
    "\t\trunning_average.append(episode_reward)\n",
    "\t\n",
    "\t\tepisode_reward = 0\n",
    "\t\tepisode_timesteps = 0\n",
    "\t\tepisode_num += 1\n",
    "\n",
    "\n",
    "\telse: # select an action from the actor network with noise\n",
    "\t\taction = policy.select_action(state, noise=True).numpy()\n",
    "\t\tif record_episode_actions:\n",
    "\t\t\trecord_episode_actions_log[episode_num].append(action)\n",
    "\n",
    "\t# the agent plays the action\n",
    "\t\tnext_state, reward, terminal, truncated, info = env.step(action)\n",
    " \n",
    "\t\tdone = terminal or truncated\n",
    "\n",
    "\t# add to the total episode reward\n",
    "\t\tepisode_reward += reward\n",
    "\n",
    "\t\tpolicy.train(observation=(state, next_state, action, reward, terminal), replay_buffer=replay_buffer)\n",
    "\n",
    "\t# update the state, episode timestep and total timestep\n",
    "\t\tstate = next_state\n",
    "\t\tepisode_timesteps += 1\n",
    "\t\ttotal_timesteps += 1\n",
    "\t\teval_counter += 1\n",
    "    \n",
    "print('Saving Details.')\n",
    "policy.save(video_render_path, agent_name, episode_num)\n",
    "\n",
    "# Continually save down all results every 100 episodes\n",
    "ep_rewards = np.concatenate(list(env.return_queue))\n",
    "ep_duration = np.concatenate(list(env.length_queue))\n",
    "ep_actions = record_episode_actions_log\n",
    "\n",
    "policy.save(video_render_path, agent_name, 'final_policy')\n",
    "\n",
    "save_object(ep_rewards, video_render_path+f\"ep_rewards_{episode_num}\")\n",
    "save_object(ep_duration, video_render_path+f\"ep_duration_{episode_num}\")\n",
    "save_object(ep_actions, video_render_path+f\"ep_actions_{episode_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Variants\n",
    "\n",
    "Other variants require minor hyperparameter updates to the code as follows:\n",
    "* Huber Loss, set loss='Huber' on creating the agent\n",
    "* Small buffer, set max_size=20_000 on creating the replay buffer\n",
    "* Hardcore mode, set hardcore=True on creating the environment\n",
    "\n",
    "Additional variants where tested with varied learning rate and delayed updated parameters, but did not converge."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Baseline_3",
   "notebookOrigID": 2090834559688895,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
